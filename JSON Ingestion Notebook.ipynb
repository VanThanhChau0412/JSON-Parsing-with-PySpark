{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f617d0e-b970-4106-b415-995022694155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "s3_folder = \"s3://applevel-dev-images/databrick_input/\"\n",
    "json_files = [f.path for f in dbutils.fs.ls(s3_folder) if f.path.endswith('.json')]\n",
    "dataframes = [spark.read.option(\"multiline\", \"true\").json(file) for file in json_files]\n",
    "\n",
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5044dc-d66c-4909-bb2b-33920b77dcbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"File number {i}\")\n",
    "    df.printSchema()\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "133b5451-fa1e-4dac-8bcf-a93294dbaad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode_outer, struct, lit\n",
    "from pyspark.sql.types import StructType, ArrayType, StringType, BooleanType\n",
    "\n",
    "def find_field_paths(schema, target_field, prefix=\"\"):\n",
    "    # Initialize an empty list to store full paths to the target field\n",
    "    paths = []\n",
    "    \n",
    "    # Loop through each field in the current schema (StructType)\n",
    "    for field in schema.fields:\n",
    "        \n",
    "        # Construct the full path for the current field\n",
    "        # If a prefix exists, prepend it with a dot (e.g., 'analytics.details')\n",
    "        # Otherwise, just use the field name\n",
    "        field_name = f\"{prefix}.{field.name}\" if prefix else field.name\n",
    "        \n",
    "        # Get the data type of the current field\n",
    "        dtype = field.dataType\n",
    "\n",
    "        # If the current field's name matches the target field\n",
    "        # (e.g., we're looking for 'aws'), add its full path to the list\n",
    "        if field.name == target_field:\n",
    "            paths.append(field_name)\n",
    "\n",
    "        # If the field is a StructType (nested object), recurse into it\n",
    "        # The prefix becomes the current field_name for nested fields\n",
    "        if isinstance(dtype, StructType):\n",
    "            paths += find_field_paths(dtype, target_field, prefix=field_name)\n",
    "        \n",
    "        # If the field is an ArrayType of StructType, recurse into elementType\n",
    "        # This handles arrays of nested objects\n",
    "        elif isinstance(dtype, ArrayType) and isinstance(dtype.elementType, StructType):\n",
    "            paths += find_field_paths(dtype.elementType, target_field, prefix=field_name)\n",
    "    \n",
    "    # Return the list of all full paths found for the target field in this schema\n",
    "    return paths\n",
    "\n",
    "#print(find_field_paths(dataframes[0].schema, \"aws\"))\n",
    "\n",
    "# --- Dynamic extractor generator ---\n",
    "def make_extractor(df, field_path):\n",
    "    \"\"\"\n",
    "    Dynamically extract a nested field from a DataFrame, handling any combination\n",
    "    of structs and arrays, and return a DataFrame with a single column 'value'.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input Spark DataFrame\n",
    "        field_path (str): Dot-separated path to the target field, e.g., \"analytics.aws\"\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Flattened DataFrame with a single column 'value' containing the target field\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the full field path into parts separated by dots\n",
    "    # Example: \"analytics.details.aws\" -> [\"analytics\", \"details\", \"aws\"]\n",
    "    parts = field_path.split(\".\")\n",
    "    \n",
    "    # Start with the original DataFrame\n",
    "    tmp_df = df\n",
    "    #print(\"This is the original schema\")\n",
    "    #tmp_df.printSchema()\n",
    "    # Loop through all parts of the path except the last field\n",
    "    # The last field is the target column we want to extract\n",
    "    for part in parts[:-1]:\n",
    "        # Get the data type of the current part\n",
    "        # tmp_df.schema.fields contains the top-level columns of the DataFrame\n",
    "        # Here, 'top-level' means columns that exist directly in the DataFrame, not nested inside a struct or array\n",
    "        #print(f'Find {part} in {tmp_df.schema}')\n",
    "        #print(f'At part = {part}')\n",
    "        dtype = next(f for f in tmp_df.schema.fields if f.name == part).dataType\n",
    "\n",
    "        # If the current part is an ArrayType:\n",
    "        # - Arrays are also considered top-level if they are direct columns\n",
    "        # - We explode the array so that each element becomes its own row\n",
    "        # - explode_outer ensures null or empty arrays are preserved (not dropped)\n",
    "        if isinstance(dtype, ArrayType):\n",
    "            tmp_df = tmp_df.withColumn(part, explode_outer(col(part)))\n",
    "            #print(f'After explode {part}')\n",
    "            #tmp_df.printSchema()\n",
    "        \n",
    "        #if (i < len(parts) - 1):\n",
    "        tmp_df = tmp_df.select(col(part + \".*\"))\n",
    "        #print(tmp_df.columns)\n",
    "\n",
    "    # Return the flattened DataFrame containing the target field as \"value\"\n",
    "    return tmp_df\n",
    "\n",
    "# --- Process single file for multiple metrics ---\n",
    "def process_df_multi_metrics(df, metrics=[\"aws\",\"azure\",\"reachable\",\"abc\",\"bcd\"]):\n",
    "    \"\"\"\n",
    "    Process a Spark DataFrame for multiple metrics and return a JSON/dict report.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input Spark DataFrame.\n",
    "        metrics (list): List of metric field names to count non-null values.\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON-style report with group_id and metric counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Detect group_id if exists\n",
    "    group_id_val = None\n",
    "    if \"group_id\" in df.columns:\n",
    "        group_id_val = df.select(col(\"group_id\").cast(\"string\")).limit(1).collect()[0][0]\n",
    "\n",
    "    report = {}\n",
    "    \n",
    "    # Loop through each metric to count non-null occurrences\n",
    "    for metric in metrics:\n",
    "        # Find all possible paths for this metric in the schema\n",
    "        paths = find_field_paths(df.schema, metric)\n",
    "        \n",
    "        # If metric not found in schema, count = 0\n",
    "        if not paths:\n",
    "            report[metric+\"_count\"] = 0\n",
    "            continue\n",
    "        \n",
    "        # Extract using the first non-empty path\n",
    "        for path in paths:\n",
    "            df = make_extractor(df, path)\n",
    "            #print(f'Extracting {metric}')\n",
    "            #display(df)\n",
    "            reachable_column = \"reachable\"\n",
    "            dtype = [f.dataType for f in df.schema.fields if f.name == reachable_column][0]\n",
    "            if metric not in [\"abc\", \"bcd\"]:\n",
    "                count_non_null = df.filter(col(metric).isNotNull()).count()\n",
    "            else:\n",
    "                count_non_null = df.filter(col(metric).isNotNull() & col(reachable_column).isNotNull()).count()\n",
    "                if dtype is not None and isinstance(dtype, StringType):\n",
    "                    count_non_null -= df.filter(col(reachable_column) == \"no\").count()\n",
    "                elif dtype is not None and isinstance(dtype, BooleanType):\n",
    "                    count_non_null -= df.filter(col(reachable_column) == False).count()\n",
    "\n",
    "            if metric == \"reachable\" and dtype is not None:\n",
    "                if isinstance(dtype, StringType):\n",
    "                    # Non-reachable = \"no\" + null\n",
    "                    non_reachable_count = df.filter(col(reachable_column) == \"no\").count()\n",
    "                elif isinstance(dtype, BooleanType):\n",
    "                    # Non-reachable = False + null\n",
    "                    non_reachable_count = df.filter(col(reachable_column) == False).count()\n",
    "                count_non_null = count_non_null - non_reachable_count\n",
    "                non_reachable_count += df.filter(col(metric).isNull()).count()\n",
    "                report[\"nonreachable\"] = non_reachable_count\n",
    "\n",
    "            if count_non_null > 0:\n",
    "                report[metric+\"_count\"] = count_non_null\n",
    "                break\n",
    "        else:\n",
    "            # If all paths are empty, set count to 0\n",
    "            report[metric+\"_count\"] = 0\n",
    "    \n",
    "    # Build final JSON-style object\n",
    "    result_json = {\n",
    "        \"group_id\": group_id_val,\n",
    "        \"analytics\": {\n",
    "            \"vulnerabilities\": {\n",
    "                \"aws_account\" : report['aws_count'],\n",
    "                \"azure_account\" : report['azure_count'],\n",
    "            }\n",
    "        },\n",
    "        \"networking\": {\n",
    "            \"operational\": {\n",
    "                \"count\": report[\"reachable_count\"],\n",
    "                \"abc\": report[\"abc_count\"],\n",
    "                \"bcd\": report[\"bcd_count\"]\n",
    "            },\n",
    "            \"non_operational_total\": report[\"nonreachable\"]\n",
    "        }\n",
    "    }   \n",
    "    \n",
    "    return result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad61103e-e907-42e2-83fb-de65615746c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#field_path = find_field_paths(dataframes[0].schema, \"aws\")[0]\n",
    "#print(f'Path to aws: {field_path}')\n",
    "#extracted = make_extractor(dataframes[0], field_path\n",
    "test_json = process_df_multi_metrics(dataframes[0])\n",
    "display(test_json)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "JSON Ingestion Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
